# ğŸš€ Production RAG Pipeline

##  Overview

This is a **production-ready, scalable RAG (Retrieval-Augmented Generation) pipeline** designed to showcase expertise in GenAI architecture, optimization, and evaluation.

### âœ¨ Key Features

- âœ… **Full OOP Architecture**: Modular, reusable components
- âœ… **Comprehensive Evaluation**: Retrieval, Generation, RAGAS, DeepEval metrics
- âœ… **Grid Search Optimization**: Systematic hyperparameter tuning
- âœ… **MLflow Tracking**: Complete experiment logging
- âœ… **Cost & Latency Optimization**: Real-time performance tracking
- âœ… **Visualization**: 3D tradeoff plots, Pareto frontiers
- âœ… **Reranker Integration**: Cross-encoder for improved accuracy
- âœ… **Golden Dataset**: 100 stratified Q&A examples
- âœ… **Google Colab Ready**: Run on free GPU
- âœ… **Modular for 4 Projects**: Shared evaluation framework

---

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAG Pipeline Architecture                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Documents  â†’  Chunking  â†’  Embedding  â†’  FAISS Index      â”‚
â”‚                                                             â”‚
â”‚  Query  â†’  Embed  â†’  Retrieve (Top-K)  â†’  Rerank (Top-N)  â”‚
â”‚                                         â†“                    â”‚
â”‚                                    LLM Generate             â”‚
â”‚                                         â†“                    â”‚
â”‚                                      Answer                  â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Evaluation Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Golden Dataset (100 Q&A)  â†’  Run Pipeline  â†’  Evaluate    â”‚
â”‚                                                             â”‚
â”‚  Metrics:                                                   â”‚
â”‚    â€¢ Retrieval: P@K, R@K, MRR, NDCG, MAP                   â”‚
â”‚    â€¢ Generation: BLEU, ROUGE, F1, EM                        â”‚
â”‚    â€¢ RAGAS: Faithfulness, Relevancy                         â”‚
â”‚    â€¢ DeepEval: Hallucination Detection                      â”‚
â”‚    â€¢ Performance: Latency, Cost                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Project Structure

```
project1_rag_production/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                    # Core RAG components (OOP)
â”‚   â”‚   â”œâ”€â”€ document_processor.py   # Chunking with metadata
â”‚   â”‚   â”œâ”€â”€ embedder.py             # Sentence transformers
â”‚   â”‚   â”œâ”€â”€ retriever.py            # FAISS vector search
â”‚   â”‚   â”œâ”€â”€ reranker.py             # Cross-encoder reranking
â”‚   â”‚   â””â”€â”€ generator.py            # LLM generation
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/                # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py         # Main RAG pipeline
â”‚   â”‚   â””â”€â”€ pipeline_factory.py     # Config-based creation
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/              # Evaluation (imports from shared)
â”‚   â”‚   â””â”€â”€ __init__.py             # Imports shared_evaluation
â”‚   â”‚
â”‚   â””â”€â”€ optimization/            # Grid search
â”‚       â””â”€â”€ grid_search_orchestrator.py
â”‚
â”œâ”€â”€ shared_evaluation/           # ğŸ”„ SHARED ACROSS 4 PROJECTS
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ retrieval_metrics.py
â”‚   â”‚   â”œâ”€â”€ generation_metrics.py
â”‚   â”‚   â”œâ”€â”€ ragas_evaluator.py
â”‚   â”‚   â””â”€â”€ deepeval_evaluator.py
â”‚   â”œâ”€â”€ composite_evaluator.py
â”‚   â”œâ”€â”€ mlflow_tracker.py
â”‚   â”œâ”€â”€ visualizer.py
â”‚   â”œâ”€â”€ export_utils.py
â”‚   â””â”€â”€ golden_dataset.py
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base_config.yaml         # Default configuration
â”‚   â””â”€â”€ grid_search_config.yaml  # Parameter grid
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_grid_search.py       # CLI: Full optimization
â”‚   â””â”€â”€ demo_single_query.py     # CLI: Single query test
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Google_Colab_Production_RAG.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/MonoRepo.git
cd MonoRepo/project1_rag_production

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
```

### 2. Single Query Demo

```bash
python scripts/demo_single_query.py \
    --config ./configs/base_config.yaml \
    --query "What is machine learning?" \
    --num-docs 50
```

### 3. Full Grid Search Optimization

```bash
python scripts/run_grid_search.py \
    --base-config ./configs/base_config.yaml \
    --grid-config ./configs/grid_search_config.yaml \
    --num-docs 100 \
    --output-dir ./outputs
```

### 4. Google Colab

Open `notebooks/Google_Colab_Production_RAG.ipynb` in Google Colab for GPU-accelerated execution.

---

## ğŸ” Core Components

### 1. Document Processor
- **Chunking strategies**: Fixed size, sentence-based
- **Metadata extraction**: Length, word count, index
- **Overlap control**: Configurable overlap between chunks

### 2. Embedder
- **Models**: Sentence Transformers (all-MiniLM, MPNet, BGE)
- **Batch processing**: GPU-accelerated
- **Normalization**: Automatic embedding normalization

### 3. Retriever
- **FAISS indexes**: Flat, HNSW, IVF
- **Metrics**: L2, Inner Product
- **Scalability**: Handles millions of vectors

### 4. Reranker â­
- **Cross-encoder models**: MS MARCO, STSB
- **Two-stage retrieval**: Broad recall â†’ Precise reranking
- **Significant accuracy boost**: +10-15% typical improvement

### 5. Generator
- **LLM support**: T5, LLaMA, Phi, Mistral
- **Quantization**: 4-bit, 8-bit for efficiency
- **Generation params**: Temperature, top-p, num_beams

---

## ğŸ“Š Evaluation Framework

### Metrics Tracked

#### Retrieval Metrics
- **Precision@K**: Relevance of top K results
- **Recall@K**: Coverage of relevant documents
- **MRR**: Mean Reciprocal Rank
- **NDCG@K**: Normalized Discounted Cumulative Gain
- **MAP**: Mean Average Precision
- **Hit Rate**: At least one relevant in top K

#### Generation Metrics
- **BLEU**: N-gram overlap with references
- **ROUGE**: Recall-oriented overlap
- **F1 Score**: Token-level precision & recall
- **Exact Match**: Exact string match

#### RAGAS Metrics
- **Faithfulness**: Answer grounded in context
- **Answer Relevancy**: Answer addresses question
- **Context Precision**: Relevant context retrieved
- **Context Recall**: All relevant context retrieved

#### DeepEval Metrics
- **Hallucination Detection**: Identifies fabricated information
- **Bias Detection**: Identifies unfair bias
- **Toxicity**: Identifies harmful content

#### Performance Metrics
- **Latency**: Total, retrieval, reranking, generation (ms)
- **Cost**: Embedding, LLM tokens ($)
- **Throughput**: Queries per second

### Composite Score

```python
composite_score = (
    0.5 * accuracy +  # Highest weight
    0.3 * (1 / latency) +  # Medium weight
    0.2 * (1 / cost)  # Lower weight
)
```

---

## ğŸ§ª Grid Search

### Strategy: One Parameter at a Time

1. **Baseline**: Run with default config
2. **Chunking**: Vary chunk_size, chunk_overlap, strategy
3. **Embedding**: Vary model, batch_size
4. **Retrieval**: Vary index_type, top_k
5. **Reranking**: Vary model, top_n, enabled/disabled
6. **LLM**: Vary model, quantization
7. **Generation**: Vary max_length, temperature, num_beams

### Configuration

Edit `configs/grid_search_config.yaml` to define parameter spaces:

```yaml
chunking_grid:
  chunk_size:
    baseline: 512
    options: [256, 512, 768, 1024]
  
  chunk_overlap:
    baseline: 100
    options: [0, 50, 100, 150, 200]
```

---

## ğŸ“ˆ Visualization & Reporting

### Outputs Generated

1. **CSV Files**:
   - `all_experiments.csv`: All runs with parameters & metrics
   - `top_10_architectures.csv`: Best configurations

2. **TXT Reports**:
   - `experiment_summary.txt`: Human-readable summary

3. **JSON**:
   - `detailed_results.json`: Complete structured results

4. **Plots**:
   - `3d_tradeoff.png`: Accuracy vs Latency vs Cost
   - `pareto_frontier.png`: Optimal tradeoffs
   - `top_architectures_comparison.png`: Side-by-side comparison
   - `metric_distributions.png`: Distribution histograms

### MLflow UI

```bash
mlflow ui --backend-store-uri ./mlflow_tracking
```

Visit http://localhost:5000 to explore experiments interactively.

---

## ğŸ”„ Modular Design: 4 Projects

The `shared_evaluation/` framework is **reusable across**:

1. **Project 1**: Basic RAG (this project)
2. **Project 2**: RAG with LLM Judge
3. **Project 3**: Agentic RAG
4. **Project 4**: Multi-modal Agent System

### Benefits

- âœ… **No code duplication**: Write once, use everywhere
- âœ… **Consistent metrics**: Same evaluation across projects
- âœ… **Easy maintenance**: Fix bugs in one place
- âœ… **Rapid development**: Focus on core logic, not boilerplate

---

## ğŸ’¡ Best Practices

### 1. Golden Dataset
- Use stratified sampling (difficulty, length, type)
- 100-500 examples recommended
- Include edge cases

### 2. Grid Search
- Start with baseline
- Change one parameter at a time
- Log everything to MLflow

### 3. Reranking
- Always enable for production
- 2-stage retrieval significantly improves accuracy
- Slight latency increase (~50-100ms) is worth it

### 4. Cost Optimization
- Use smaller models for embedding (all-MiniLM)
- Apply quantization for LLMs (4-bit)
- Cache embeddings when possible

### 5. Latency Optimization
- Batch queries when possible
- Use HNSW index for large corpora
- Limit top_k to necessary size

---

## ğŸ“ Technical Highlights:

### Architecture Expertise
- âœ… Clean OOP design with SOLID principles
- âœ… Factory pattern for configuration-based instantiation
- âœ… Modular components with clear interfaces
- âœ… Separation of concerns (pipeline vs evaluation vs optimization)

### Optimization Expertise
- âœ… Systematic hyperparameter tuning (grid search)
- âœ… Multi-objective optimization (accuracy vs latency vs cost)
- âœ… Pareto frontier analysis
- âœ… Composite ranking scores

### Evaluation Expertise
- âœ… Comprehensive metric suite (retrieval + generation + frameworks)
- âœ… Multiple evaluation frameworks (RAGAS, DeepEval)
- âœ… Ranking & non-ranking metrics
- âœ… Golden dataset methodology

### Production Readiness
- âœ… MLflow experiment tracking
- âœ… Complete logging & monitoring
- âœ… Cost & latency tracking
- âœ… CSV/JSON exports for analysis
- âœ… CLI & notebook interfaces
- âœ… Google Colab compatible

### No Hallucination Focus
- âœ… Faithfulness metrics (RAGAS)
- âœ… Hallucination detection (DeepEval)
- âœ… Context grounding validation
- âœ… Reranker for precision

---

## ğŸ“š References

- **FAISS**: [facebook/faiss](https://github.com/facebookresearch/faiss)
- **Sentence Transformers**: [UKPLab/sentence-transformers](https://github.com/UKPLab/sentence-transformers)
- **RAGAS**: [explodinggradients/ragas](https://github.com/explodinggradients/ragas)
- **DeepEval**: [confident-ai/deepeval](https://github.com/confident-ai/deepeval)
- **MLflow**: [mlflow.org](https://mlflow.org/)

---

## ğŸ“ Contact

**Author**: Christian Dudziak  
**GitHub**: [GitHub](https://github.com/chris-conifer)  
**LinkedIn**: [LinkedIn](https://www.linkedin.com/in/christian-dudziak-b9193931/)  

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

- HuggingFace for datasets and models
- Facebook Research for FAISS
- The open-source community for amazing tools

---

**â­ If you find this project useful, please star it on GitHub!**



