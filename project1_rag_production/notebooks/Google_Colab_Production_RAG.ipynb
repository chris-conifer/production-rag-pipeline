{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de7b75a0",
   "metadata": {},
   "source": [
    "# üöÄ Production RAG Pipeline - Google Colab\n",
    "\n",
    "**100% FREE & Open Source - No API Keys Required!**\n",
    "\n",
    "This notebook runs a complete RAG pipeline with:\n",
    "- ‚úÖ Hybrid Retrieval (Dense + BM25)\n",
    "- ‚úÖ BGE Reranker (#1 on MTEB Leaderboard)\n",
    "- ‚úÖ Systematic Grid Search\n",
    "- ‚úÖ MLflow Experiment Tracking\n",
    "- ‚úÖ Final Evaluation on Golden Test Set\n",
    "\n",
    "---\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "1. **Enable GPU**: `Runtime` ‚Üí `Change runtime type` ‚Üí `T4 GPU` ‚Üí `Save`\n",
    "2. **Run cells in order** (top to bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Setup Environment\n",
    "import os\n",
    "\n",
    "# Clone the repository (replace YOUR_USERNAME with your GitHub username)\n",
    "!git clone https://github.com/YOUR_USERNAME/production-rag-pipeline.git\n",
    "%cd production-rag-pipeline\n",
    "\n",
    "# Install PyTorch with CUDA\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install RAG dependencies\n",
    "!pip install -q transformers sentence-transformers datasets faiss-cpu\n",
    "!pip install -q mlflow pandas pyyaml tqdm rouge-score nltk rank-bm25\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected - will run on CPU (slower)\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Run Grid Search (Phase 1 - Hyperparameter Tuning)\n",
    "%cd project1_rag_production\n",
    "\n",
    "# Run grid search with:\n",
    "# - 300 documents from SQuAD dataset\n",
    "# - 30 Q&A pairs for evaluation\n",
    "# - Max 10 experiment configurations\n",
    "\n",
    "!python scripts/run_grid_search.py \\\n",
    "    --num-docs 300 \\\n",
    "    --num-qa 30 \\\n",
    "    --max-experiments 10\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 Complete! Best configuration identified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd97b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. View Grid Search Results\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: GRID SEARCH RESULTS (Ranked by Composite Score)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df = pd.read_csv(\"outputs/grid_search_results.csv\")\n",
    "\n",
    "# Display key columns\n",
    "display_cols = ['name', 'correct', 'hallucination', 'quality_score', \n",
    "                'gen_f1_score', 'ret_mrr', 'composite_score']\n",
    "available_cols = [c for c in display_cols if c in df.columns]\n",
    "\n",
    "print(df[available_cols].head(10).to_string())\n",
    "\n",
    "# Best configuration\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "best = df.iloc[0]\n",
    "print(f\"Name: {best['name']}\")\n",
    "print(f\"Composite Score: {best['composite_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8021262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Run Final Evaluation (Phase 2 - Golden Test Set)\n",
    "# Uses the BEST configuration from Phase 1 automatically\n",
    "# Evaluates on SQuAD validation set (never seen during tuning)\n",
    "\n",
    "!python scripts/run_final_evaluation.py --num-docs 500 --num-qa 50\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2 Complete! Unbiased metrics on golden test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. View Final Evaluation Report\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: FINAL EVALUATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find the latest report\n",
    "report_dir = \"outputs/phase2_final_evaluation\"\n",
    "if os.path.exists(report_dir):\n",
    "    reports = sorted(glob.glob(f\"{report_dir}/FINAL_EVALUATION_REPORT_*.txt\"))\n",
    "    if reports:\n",
    "        latest_report = reports[-1]\n",
    "        print(f\"Latest Report: {os.path.basename(latest_report)}\\n\")\n",
    "        with open(latest_report, 'r') as f:\n",
    "            print(f.read())\n",
    "    else:\n",
    "        print(\"No reports found. Run Phase 2 first.\")\n",
    "else:\n",
    "    print(\"Phase 2 not run yet. Execute Cell 4 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. View MLflow Experiment Logs\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"outputs/mlflow_tracking\")\n",
    "\n",
    "# Get all experiments\n",
    "experiments = mlflow.search_experiments()\n",
    "print(\"=\" * 70)\n",
    "print(\"MLFLOW EXPERIMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\nExperiment: {exp.name}\")\n",
    "    print(f\"  ID: {exp.experiment_id}\")\n",
    "    \n",
    "    # Get runs for this experiment\n",
    "    runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "    if len(runs) > 0:\n",
    "        print(f\"  Total Runs: {len(runs)}\")\n",
    "        print(f\"\\n  Top 5 Runs by Composite Score:\")\n",
    "        \n",
    "        if 'metrics.composite_score' in runs.columns:\n",
    "            top_runs = runs.nlargest(5, 'metrics.composite_score')\n",
    "            for i, (_, run) in enumerate(top_runs.iterrows(), 1):\n",
    "                print(f\"    {i}. Run {run['run_id'][:8]}...\")\n",
    "                print(f\"       Composite Score: {run.get('metrics.composite_score', 0):.4f}\")\n",
    "                print(f\"       Quality Score: {run.get('metrics.quality_score', 0):.4f}\")\n",
    "                print(f\"       F1 Score: {run.get('metrics.gen_f1_score', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa663d",
   "metadata": {},
   "source": [
    "## üìä All Logged Metrics\n",
    "\n",
    "The following metrics are tracked for each experiment run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. View All Logged Metrics & Parameters\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"outputs/mlflow_tracking\")\n",
    "\n",
    "# Get all runs\n",
    "experiments = mlflow.search_experiments()\n",
    "if experiments:\n",
    "    exp_id = experiments[0].experiment_id\n",
    "    runs = mlflow.search_runs(experiment_ids=[exp_id])\n",
    "    \n",
    "    # Select metric columns\n",
    "    metric_cols = [c for c in runs.columns if c.startswith('metrics.')]\n",
    "    param_cols = [c for c in runs.columns if c.startswith('params.')]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ALL LOGGED METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    for col in sorted(metric_cols):\n",
    "        print(f\"  ‚Ä¢ {col.replace('metrics.', '')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ALL LOGGED PARAMETERS\")\n",
    "    print(\"=\" * 70)\n",
    "    for col in sorted(param_cols):\n",
    "        print(f\"  ‚Ä¢ {col.replace('params.', '')}\")\n",
    "else:\n",
    "    print(\"No experiments found. Run grid search first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab417242",
   "metadata": {},
   "source": [
    "## üíæ Download Results\n",
    "\n",
    "Download all results including:\n",
    "- Grid search results (CSV)\n",
    "- MLflow tracking logs\n",
    "- Final evaluation reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5df627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Download All Results\n",
    "from google.colab import files\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Create zip of all outputs\n",
    "if os.path.exists('outputs'):\n",
    "    shutil.make_archive('rag_pipeline_results', 'zip', 'outputs')\n",
    "    \n",
    "    # Download\n",
    "    files.download('rag_pipeline_results.zip')\n",
    "    \n",
    "    print(\"‚úÖ Results downloaded!\")\n",
    "    print(\"   Contains:\")\n",
    "    print(\"   - grid_search_results.csv\")\n",
    "    print(\"   - MLflow tracking logs\")\n",
    "    print(\"   - Phase 2 evaluation reports\")\n",
    "else:\n",
    "    print(\"No outputs found. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973367ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9. Compare All Experiment Runs (Table View)\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.set_tracking_uri(\"outputs/mlflow_tracking\")\n",
    "\n",
    "experiments = mlflow.search_experiments()\n",
    "if experiments:\n",
    "    exp_id = experiments[0].experiment_id\n",
    "    runs = mlflow.search_runs(experiment_ids=[exp_id])\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_cols = [\n",
    "        'params.config_name',\n",
    "        'metrics.quality_score',\n",
    "        'metrics.hallucination_rate', \n",
    "        'metrics.gen_f1_score',\n",
    "        'metrics.ret_mrr',\n",
    "        'metrics.composite_score'\n",
    "    ]\n",
    "    available = [c for c in comparison_cols if c in runs.columns]\n",
    "    \n",
    "    if available:\n",
    "        comparison_df = runs[available].copy()\n",
    "        comparison_df.columns = [c.replace('params.', '').replace('metrics.', '') for c in available]\n",
    "        comparison_df = comparison_df.sort_values('composite_score', ascending=False)\n",
    "        print(comparison_df.to_string())\n",
    "else:\n",
    "    print(\"No experiments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 10. Export MLflow Data to CSV\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.set_tracking_uri(\"outputs/mlflow_tracking\")\n",
    "\n",
    "experiments = mlflow.search_experiments()\n",
    "if experiments:\n",
    "    all_runs = []\n",
    "    for exp in experiments:\n",
    "        runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
    "        all_runs.append(runs)\n",
    "    \n",
    "    if all_runs:\n",
    "        full_df = pd.concat(all_runs, ignore_index=True)\n",
    "        full_df.to_csv(\"outputs/mlflow_all_runs_export.csv\", index=False)\n",
    "        print(f\"‚úÖ Exported {len(full_df)} runs to outputs/mlflow_all_runs_export.csv\")\n",
    "        print(f\"   Columns: {len(full_df.columns)}\")\n",
    "else:\n",
    "    print(\"No experiments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965212c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 11. Quick Tips\n",
    "\n",
    "print(\"\"\"\n",
    "================================================================================\n",
    "QUICK TIPS\n",
    "================================================================================\n",
    "\n",
    "üîß CUSTOMIZE GRID SEARCH:\n",
    "   Edit: configs/grid_search_config.yaml\n",
    "   \n",
    "   Example changes:\n",
    "   - Add more chunk sizes: options: [256, 512, 768, 1024]\n",
    "   - Test different rerankers: options: [\"bge\", \"cross_encoder\"]\n",
    "   - Increase top_k: options: [5, 10, 15, 20]\n",
    "\n",
    "üìä UNDERSTAND METRICS:\n",
    "   - quality_score: % of correct answers\n",
    "   - hallucination_rate: % of fabricated answers (lower = better)\n",
    "   - gen_f1_score: Token-level match with ground truth\n",
    "   - ret_mrr: Mean Reciprocal Rank (retrieval quality)\n",
    "   - composite_score: Weighted combination (what we optimize for)\n",
    "\n",
    "üí° REDUCE MEMORY USAGE:\n",
    "   - Use fewer documents: --num-docs 100\n",
    "   - Use fewer Q&A pairs: --num-qa 20\n",
    "   - Reduce max experiments: --max-experiments 5\n",
    "\n",
    "üöÄ BEST PRACTICES:\n",
    "   1. Run grid search with small data first (--num-docs 100)\n",
    "   2. Review results to identify promising configurations\n",
    "   3. Run final evaluation with more data (--num-docs 500)\n",
    "   4. Compare metrics to validate improvements\n",
    "\n",
    "================================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebdef01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Complete!\n",
    "\n",
    "You've successfully run a production RAG pipeline with:\n",
    "\n",
    "| Phase | What It Does |\n",
    "|-------|-------------|\n",
    "| **Phase 1** | Grid search to find optimal hyperparameters |\n",
    "| **Phase 2** | Final evaluation on held-out test data |\n",
    "\n",
    "### Key Metrics Explained\n",
    "\n",
    "| Metric | What It Measures |\n",
    "|--------|------------------|\n",
    "| **Accuracy** | % of questions answered correctly |\n",
    "| **Hallucination Rate** | % of fabricated answers (lower is better) |\n",
    "| **F1 Score** | Token overlap with ground truth |\n",
    "| **MRR** | How high the correct document ranks |\n",
    "| **Hit Rate@3** | Is correct doc in top 3? |\n",
    "| **Composite Score** | Weighted combination of all metrics |\n",
    "\n",
    "---\n",
    "\n",
    "**All models are FREE and open source - no API keys needed!**\n",
    "\n",
    "‚≠ê Star this repo if you found it useful!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
